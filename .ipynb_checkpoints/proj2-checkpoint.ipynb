{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools  \n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.core.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "happy-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(audio, n_noise_samples=1000, noise_factor=1.0, mean_filter_size=100):\n",
    "    \"\"\" Removes the silence at the beginning and end of the passed audio data\n",
    "    Fits noise based on the last n_noise_samples samples in the period\n",
    "    Finds where the mean-filtered magnitude > noise\n",
    "    :param audio: numpy array of audio\n",
    "    :return: a trimmed numpy array\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    end = len(audio)-1\n",
    "\n",
    "    mag = abs(audio)\n",
    "\n",
    "    noise_sample_period = mag[end-n_noise_samples:end]\n",
    "    noise_threshold = noise_sample_period.max()*noise_factor\n",
    "\n",
    "    mag_mean = np.convolve(mag, [1/float(mean_filter_size)]*mean_filter_size, 'same')\n",
    "\n",
    "    # find onset\n",
    "    for idx, point in enumerate(mag_mean):\n",
    "        if point > noise_threshold:\n",
    "            start = idx\n",
    "            break\n",
    "\n",
    "    # Reverse the array for trimming the end\n",
    "    for idx, point in enumerate(mag_mean[::-1]):\n",
    "        if point > noise_threshold:\n",
    "            end = len(audio) - idx\n",
    "            break\n",
    "\n",
    "    return audio[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "working-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patient-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_spectrogram(waveform):\n",
    "  # Padding for files with less than 16000 samples\n",
    "  zero_padding = tf.zeros([max_size] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length\n",
    "  waveform = tf.cast(waveform, tf.float32)\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleasant-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram(waveform):\n",
    "    spec = librosa.feature.melspectrogram(y=waveform, sr=8000)\n",
    "    return librosa.power_to_db(spec, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oriented-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_spectrogram(waveform):\n",
    "    return librosa.feature.mfcc(y=waveform, sr=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lesser-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  # Convert to frequencies to log scale and transpose so that the time is\n",
    "  # represented in the x-axis (columns).\n",
    "  log_spec = np.log(spectrogram.T)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clean-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Reds')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.1f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "variable-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_white_noise(signal,SNR) :\n",
    "    RMS_s=math.sqrt(np.mean(signal.astype(np.int32)**2))\n",
    "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
    "    STD_n=RMS_n\n",
    "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dimensional-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):  \n",
    "    newdata = []\n",
    "    for audio in data:\n",
    "        noise = get_white_noise(audio, 3)\n",
    "        newdata.append(audio + noise)\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prostate-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pitch(data): \n",
    "    newdata = []\n",
    "    for audio in data:\n",
    "        newdata.append(librosa.effects.pitch_shift(audio.astype(np.float64), 8000, 2))\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(data): \n",
    "    newdata = []\n",
    "    for audio in data:\n",
    "        newdata.append( librosa.effects.time_stretch(audio.astype(np.float64), 0.75) )\n",
    "        newdata.append( librosa.effects.time_stretch(audio.astype(np.float64), 1.5) )\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "weighted-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(X, y, *, noise=False, pitch=False, time_stretch=False):\n",
    "    \n",
    "    if pitch:\n",
    "        print('Pitch')\n",
    "        X += change_pitch(X)\n",
    "        y *= 2  \n",
    "      \n",
    "    if time_stretch:\n",
    "        print('Time Stretch')\n",
    "        X += stretch(X[50:])\n",
    "        y *= 4\n",
    "        \n",
    "    if noise:\n",
    "        print('Noise')\n",
    "        X += add_noise(X) \n",
    "        y *= 2\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-adobe",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sophisticated-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 'recordings/'\n",
    "ds_files = listdir(files)\n",
    "\n",
    "labels = [str(i) for i in range(10)]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for file in ds_files:\n",
    "    label = int(file.split(\"_\")[0])\n",
    "    rate, data = wavfile.read(join(files, file))\n",
    "    #print(data.shape)\n",
    "    trimmed_data = trim_silence(data.astype(np.float16))\n",
    "    X.append(trimmed_data)\n",
    "    y.append(label)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "casual-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input signal length=1 is too small to resample from 8979.696386474985->8000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9aa15e3f5688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_stretch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-7f35c0cd3085>\u001b[0m in \u001b[0;36mdata_augmentation\u001b[0;34m(X, y, noise, pitch, time_stretch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pitch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchange_pitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ca4fc72ebbc2>\u001b[0m in \u001b[0;36mchange_pitch\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnewdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpitch_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.7/site-packages/librosa/effects.py\u001b[0m in \u001b[0;36mpitch_shift\u001b[0;34m(y, sr, n_steps, bins_per_octave, res_type, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Stretch in time, then resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     y_shift = core.resample(\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mtime_stretch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.7/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         raise ValueError('Input signal length={} is too small to '\n\u001b[0;32m---> 98\u001b[0;31m                          'resample from {}->{}'.format(x.shape[axis], sr_orig, sr_new))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Preserve contiguity of input (if it exists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input signal length=1 is too small to resample from 8979.696386474985->8000"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "n_samples = len(X)\n",
    "n_augmentations = 2\n",
    "\n",
    "\n",
    "X, y = data_augmentation(X, y, noise=True, pitch=True, time_stretch=True)\n",
    "\n",
    "\n",
    "#display(ipd.Audio(X[s], rate=8000))\n",
    "#for i in range(0, 4):\n",
    "#    display(ipd.Audio(X[s + 2**i*n_samples], rate=8000))\n",
    "    \n",
    "    \n",
    "# padding to max size\n",
    "max_size = 0\n",
    "for x in X:\n",
    "    max_size = max(max_size, x.shape[0])\n",
    "    \n",
    "X = [ np.pad(x, (0, max_size - x.shape[0])) for x in X ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ipd.Audio( librosa.effects.time_stretch(X[s].astype(np.float64), 0.75) , rate=8000 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.60\n",
    "validation_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librosa.feature.mfcc(y_cut, n_fft=n_fft, hop_length=512,n_mfcc=128)\n",
    "\n",
    "#spec = librosa.stft(X[1500])\n",
    "sr = 8000\n",
    "S = librosa.feature.melspectrogram(X[1500], sr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = X[0]\n",
    "spectrogram = stft_spectrogram(waveform)\n",
    "\n",
    "\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "axes[0].plot(timescale, waveform)\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, max_size])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-gamma",
   "metadata": {},
   "source": [
    "### Convert Data to Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different spec lambdas\n",
    "spec = lambda X: np.array([mfcc_spectrogram(x) for x in X])\n",
    "\n",
    "\n",
    "x_train_cur = spec(x_train)\n",
    "x_val_cur = spec(x_val)\n",
    "x_test_cur = spec(x_test)\n",
    "\n",
    "print(x_train_cur.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-christian",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train_cur.shape\n",
    "num_labels = 10\n",
    "\n",
    "print(input_shape)\n",
    "\n",
    "#norm_layer = preprocessing.Normalization()\n",
    "#norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(*x_train_cur[0].shape, 1)),\n",
    "    preprocessing.Resizing(32, 32), \n",
    "    #norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "history = model.fit(\n",
    "    x_train_cur, \n",
    "    np.array(y_train),\n",
    "    validation_data=(x_val_cur, np.array(y_val)),  \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-essex",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_cur)\n",
    "\n",
    "predicted_categories = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "true_categories = y_test\n",
    "\n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "plot_confusion_matrix(cm, labels, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-pilot",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio():\n",
    "    import pyaudio\n",
    "    import wave\n",
    "\n",
    "    chunk = 1024  # Record in chunks of 1024 samples\n",
    "    sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "    channels = 1\n",
    "    fs = 8000  # Record at 44100 samples per second\n",
    "    seconds = 2\n",
    "    filename = \"output.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "    print('Recording')\n",
    "\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=fs,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "\n",
    "    frames = []  # Initialize array to store frames\n",
    "\n",
    "    # Store data in chunks for 3 seconds\n",
    "    for i in range(0, int(fs / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    # Stop and close the stream \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    # Terminate the PortAudio interface\n",
    "    p.terminate()\n",
    "\n",
    "    print('Finished recording')\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "#record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read(\"output.wav\")\n",
    "print(data.shape)\n",
    "\n",
    "trimmed_data = trim_silence(data.astype(np.float16))\n",
    "\n",
    "display(ipd.Audio(trimmed_data, rate=8000))\n",
    "\n",
    "print(trimmed_data.shape)\n",
    "\n",
    "print(max_size, trimmed_data.shape[0])\n",
    "\n",
    "my_X = np.pad(trimmed_data, (0, max_size - trimmed_data.shape[0]))\n",
    "\n",
    "my_spec = librosa.feature.mfcc(y=my_X, sr=8000)\n",
    "print(my_spec.shape)\n",
    "\n",
    "'''\n",
    "y_pred = model.predict(my_spec.reshape(1, *my_spec.shape, 1))\n",
    "predicted_categories = tf.argmax(y_pred, axis=1)\n",
    "print(predicted_categories)\n",
    "'''\n",
    "\n",
    "prediction = model(my_spec.reshape(1,*my_spec.shape, 1))\n",
    "plt.bar(labels, tf.nn.softmax(prediction[0]))\n",
    "plt.title(f'Predictions for one')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-bryan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = 'new_recordings/'\n",
    "ds_files = listdir(files)\n",
    "ds_files.sort()\n",
    "samples_per_label = 2\n",
    "ds_chunks = [ds_files[x:x+samples_per_label] for x in range(0, len(ds_files), samples_per_label)]\n",
    "\n",
    "\n",
    "predicted_categories = []\n",
    "true_categories = []\n",
    "\n",
    "for chunk in ds_chunks:\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=samples_per_label, squeeze=False)\n",
    "    \n",
    "    for file in chunk:\n",
    "        label = int(file.split(\"_\")[0])\n",
    "        sample = int(file.split(\"_\")[2].split('.')[0])\n",
    "        \n",
    "        ax = axes[0][sample]\n",
    "        ax.set_ylim([0,1])\n",
    "\n",
    "        rate, data = wavfile.read(join(files, file))\n",
    "\n",
    "        trimmed_data = trim_silence(data.astype(np.float16))\n",
    "\n",
    "        display(ipd.Audio(trimmed_data, rate=8000))\n",
    "\n",
    "        x = np.pad(trimmed_data, (0, max_size - trimmed_data.shape[0]))\n",
    "\n",
    "        x_spec = librosa.feature.mfcc(y=x, sr=8000)\n",
    "\n",
    "        \n",
    "        (prediction,) = model(x_spec.reshape(1, *x_spec.shape, 1))\n",
    "        ax.bar(labels, tf.nn.softmax(prediction))\n",
    "        ax.set_title(f'Prediction for {label} #{sample}')\n",
    "        \n",
    "        predicted_categories.append(tf.argmax(prediction))\n",
    "        true_categories.append(label)\n",
    "        \n",
    "    fig.set_size_inches(5, 2.5, forward=True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "plot_confusion_matrix(cm, labels, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-lobby",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
