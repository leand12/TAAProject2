{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "centered-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools  \n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.core.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(audio, n_noise_samples=1000, noise_factor=1.0, mean_filter_size=100):\n",
    "    \"\"\" Removes the silence at the beginning and end of the passed audio data\n",
    "    Fits noise based on the last n_noise_samples samples in the period\n",
    "    Finds where the mean-filtered magnitude > noise\n",
    "    :param audio: numpy array of audio\n",
    "    :return: a trimmed numpy array\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    end = len(audio)-1\n",
    "\n",
    "    mag = abs(audio)\n",
    "\n",
    "    noise_sample_period = mag[end-n_noise_samples:end]\n",
    "    noise_threshold = noise_sample_period.max()*noise_factor\n",
    "\n",
    "    mag_mean = np.convolve(mag, [1/float(mean_filter_size)]*mean_filter_size, 'same')\n",
    "\n",
    "    # find onset\n",
    "    for idx, point in enumerate(mag_mean):\n",
    "        if point > noise_threshold:\n",
    "            start = idx\n",
    "            break\n",
    "\n",
    "    # Reverse the array for trimming the end\n",
    "    for idx, point in enumerate(mag_mean[::-1]):\n",
    "        if point > noise_threshold:\n",
    "            end = len(audio) - idx\n",
    "            break\n",
    "\n",
    "    return audio[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_spectrogram(waveform):\n",
    "  # Padding for files with less than 16000 samples\n",
    "  zero_padding = tf.zeros([max_size] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length\n",
    "  waveform = tf.cast(waveform, tf.float32)\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seeing-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram(waveform):\n",
    "    spec = librosa.feature.melspectrogram(y=waveform, sr=8000)\n",
    "    return librosa.power_to_db(spec, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "correct-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_spectrogram(waveform):\n",
    "    return librosa.feature.mfcc(y=waveform, sr=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "individual-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  # Convert to frequencies to log scale and transpose so that the time is\n",
    "  # represented in the x-axis (columns).\n",
    "  log_spec = np.log(spectrogram.T)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "native-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Reds')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.1f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "relative-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_white_noise(signal,SNR) :\n",
    "    RMS_s=math.sqrt(np.mean(signal.astype(np.int32)**2))\n",
    "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
    "    STD_n=RMS_n\n",
    "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bearing-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):  \n",
    "    newdata = []\n",
    "    for audio in data:\n",
    "        noise = get_white_noise(audio, 2)\n",
    "        newdata.append(audio + noise)\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floating-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pitch(data): \n",
    "    newdata = []\n",
    "    for audio in data:\n",
    "        newdata.append(librosa.effects.pitch_shift(audio.astype(np.float64), 8000, 2))\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "distinct-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(X, y, noise=False, pitch=False, time_shift=False):\n",
    "    totaldata = X\n",
    "    if noise:\n",
    "        totaldata += add_noise(totaldata) \n",
    "        y += list(y)\n",
    "    \n",
    "    if pitch:\n",
    "        totaldata += change_pitch(totaldata)\n",
    "        y += list(y)\n",
    "        \n",
    "    return totaldata, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-entity",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legendary-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998\n"
     ]
    }
   ],
   "source": [
    "files = 'recordings/'\n",
    "ds_files = listdir(files)\n",
    "\n",
    "labels = [str(i) for i in range(10)]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "s = []\n",
    "\n",
    "for file in ds_files:\n",
    "    label = int(file.split(\"_\")[0])\n",
    "    rate, data = wavfile.read(join(files, file))\n",
    "    \n",
    "    #print(data.shape)\n",
    "    trimmed_data = trim_silence(data.astype(np.float16))\n",
    "    X.append(trimmed_data)\n",
    "    y.append(label)\n",
    "    \n",
    "    s.append(trimmed_data.shape[0])\n",
    "    \n",
    "\n",
    "max_size = np.max(s)\n",
    "\n",
    "print(max_size)\n",
    "\n",
    "# pad\n",
    "X = [ np.pad(x, (0, max_size - x.shape[0])) for x in X ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_augmentation(X, y, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.60\n",
    "validation_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librosa.feature.mfcc(y_cut, n_fft=n_fft, hop_length=512,n_mfcc=128)\n",
    "\n",
    "#spec = librosa.stft(X[1500])\n",
    "sr = 8000\n",
    "S = librosa.feature.melspectrogram(X[1500], sr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = X[0]\n",
    "spectrogram = stft_spectrogram(waveform)\n",
    "\n",
    "\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "axes[0].plot(timescale, waveform)\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, max_size])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-lebanon",
   "metadata": {},
   "source": [
    "### Convert Data to Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different spec lambdas\n",
    "spec = lambda X: np.array([mfcc_spectrogram(x) for x in X])\n",
    "\n",
    "\n",
    "x_train_cur = spec(x_train)\n",
    "x_val_cur = spec(x_val)\n",
    "x_test_cur = spec(x_test)\n",
    "\n",
    "print(x_train_cur.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-headquarters",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train_cur.shape\n",
    "num_labels = 10\n",
    "\n",
    "print(input_shape)\n",
    "\n",
    "#norm_layer = preprocessing.Normalization()\n",
    "#norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(*x_train_cur[0].shape, 1)),\n",
    "    preprocessing.Resizing(32, 32), \n",
    "    #norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "history = model.fit(\n",
    "    x_train_cur, \n",
    "    np.array(y_train),\n",
    "    validation_data=(x_val_cur, np.array(y_val)),  \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-harris",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_cur)\n",
    "\n",
    "predicted_categories = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "true_categories = y_test\n",
    "\n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "plot_confusion_matrix(cm, labels, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-canada",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio():\n",
    "    import pyaudio\n",
    "    import wave\n",
    "\n",
    "    chunk = 1024  # Record in chunks of 1024 samples\n",
    "    sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "    channels = 1\n",
    "    fs = 8000  # Record at 44100 samples per second\n",
    "    seconds = 2\n",
    "    filename = \"output.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "    print('Recording')\n",
    "\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=fs,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "\n",
    "    frames = []  # Initialize array to store frames\n",
    "\n",
    "    # Store data in chunks for 3 seconds\n",
    "    for i in range(0, int(fs / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    # Stop and close the stream \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    # Terminate the PortAudio interface\n",
    "    p.terminate()\n",
    "\n",
    "    print('Finished recording')\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "#record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read(\"output.wav\")\n",
    "print(data.shape)\n",
    "\n",
    "trimmed_data = trim_silence(data.astype(np.float16))\n",
    "\n",
    "display(ipd.Audio(trimmed_data, rate=8000))\n",
    "\n",
    "print(trimmed_data.shape)\n",
    "\n",
    "print(max_size, trimmed_data.shape[0])\n",
    "\n",
    "my_X = np.pad(trimmed_data, (0, max_size - trimmed_data.shape[0]))\n",
    "\n",
    "my_spec = librosa.feature.mfcc(y=my_X, sr=8000)\n",
    "print(my_spec.shape)\n",
    "\n",
    "'''\n",
    "y_pred = model.predict(my_spec.reshape(1, *my_spec.shape, 1))\n",
    "predicted_categories = tf.argmax(y_pred, axis=1)\n",
    "print(predicted_categories)\n",
    "'''\n",
    "\n",
    "prediction = model(my_spec.reshape(1,*my_spec.shape, 1))\n",
    "plt.bar(labels, tf.nn.softmax(prediction[0]))\n",
    "plt.title(f'Predictions for one')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-county",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = 'new_recordings/'\n",
    "ds_files = listdir(files)\n",
    "ds_files.sort()\n",
    "samples_per_label = 2\n",
    "ds_chunks = [ds_files[x:x+samples_per_label] for x in range(0, len(ds_files), samples_per_label)]\n",
    "\n",
    "\n",
    "predicted_categories = []\n",
    "true_categories = []\n",
    "\n",
    "for chunk in ds_chunks:\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=samples_per_label, squeeze=False)\n",
    "    \n",
    "    for file in chunk:\n",
    "        label = int(file.split(\"_\")[0])\n",
    "        sample = int(file.split(\"_\")[2].split('.')[0])\n",
    "        \n",
    "        ax = axes[0][sample]\n",
    "        ax.set_ylim([0,1])\n",
    "\n",
    "        rate, data = wavfile.read(join(files, file))\n",
    "\n",
    "        trimmed_data = trim_silence(data.astype(np.float16))\n",
    "\n",
    "        display(ipd.Audio(trimmed_data, rate=8000))\n",
    "\n",
    "        x = np.pad(trimmed_data, (0, max_size - trimmed_data.shape[0]))\n",
    "\n",
    "        x_spec = librosa.feature.mfcc(y=x, sr=8000)\n",
    "\n",
    "        \n",
    "        (prediction,) = model(x_spec.reshape(1, *x_spec.shape, 1))\n",
    "        ax.bar(labels, tf.nn.softmax(prediction))\n",
    "        ax.set_title(f'Prediction for {label} #{sample}')\n",
    "        \n",
    "        predicted_categories.append(tf.argmax(prediction))\n",
    "        true_categories.append(label)\n",
    "        \n",
    "    fig.set_size_inches(5, 2.5, forward=True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "plot_confusion_matrix(cm, labels, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-national",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
